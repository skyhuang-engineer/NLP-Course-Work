{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSO560 Homework 4 \n",
    "\n",
    "## Xiangyu Huang\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify **three pairs of documents** in the McDonalds review dataset that have over 0.85 cosine similarity using average token word2vec embeddings from spacy.\n",
    "\n",
    "2. Using the `SMS_test` and `SMS_train` datasets, build a classification model (you can simply use the `sklearn.linear_model.LogisticRegression` model used. Please attempt at least two of the vectorization techniques below:\n",
    "* `CountVectorization`\n",
    "* `TfIdfVectorization`\n",
    "* `word2vec` spacy document-level vectors\n",
    "\n",
    "Make sure you perform the following:\n",
    "* use train/test split\n",
    "* use proper model evaluation metrics\n",
    "* text preprocessing (regex, stemming/lemmatization, stopword removal, grouping entities, etc.)\n",
    "\n",
    "A discussion of the following:\n",
    "* **What techniques** you tried to improve the performance of your model.\n",
    "* What you would try to do, given more time, that would improve the performance of your model.\n",
    "* Provide an example of two **error cases** - a false positive and a false negative - that your model got wrong, and why the model did not predict the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt \n",
    "import spacy\n",
    "\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span\n",
    "from scipy.spatial.distance import cosine\n",
    "from itertools import product\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/huangsky/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/huangsky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/huangsky/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd_review = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding = 'latin-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcd_review['review'] = mcd_review['review'].str.lower()\n",
    "\n",
    "mcd_review['review'] = mcd_review['review'].str.replace(r\"\\b(\\w+)?burgers?\\b|\\b(\\w+)?sandwich(\\w+)?\\b|\\b(?:ham|chesse|steak|chicken|double|triple)s?(?:\\s)(?:burger|bun|sandwich)s?\\b\" ,\n",
    "                                                        'burger')\n",
    "\n",
    "mcd_review['review'] = mcd_review['review'].str.replace(r\"\\bmcds?\\b|\\bmcdonald's\\b|\\bmacd\\b|\\bmc(\\w+)?\\b|\\b(\\w+)?mcdonald?(\\w+)?\\b\" , \n",
    "                                                        'macdonald')\n",
    "mcd_review['review'] = mcd_review['review'].str.replace(r\"[,!.]\" ,  '')\n",
    "reviews = mcd_review['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'in', 'out', 'on', 'off', 'over', 'under', 'further', 'once', 'here', 'there', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'only', 'own', 'same', 'so', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'isn', 'ma', 'won', 'so', \"i'm\", \"i've\", \"i'll\", 'really', \"that's\", \"he'll\", \"she'll\", 'when', 'been', \"we'll\", \"they'll\", 'would', 'even', 'could', 'may']\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "remove_stop_words = [\"not\", \"nor\"] \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "sentence = \" \".join(stop_words)\n",
    "remove_stop_words.extend(re.findall(r\"\\b(\\w+n't|\\w{3,}n)\\b\", sentence))\n",
    "remove_stop_words\n",
    "for each in remove_stop_words:\n",
    "    stop_words.remove(each)\n",
    "my_stopwords = [\"so\", \"i'm\", \"i've\" ,\"i'll\",\"really\",\"that's\",\"he'll\", \"she'll\", \"when\", \"been\", \"we'll\", \"they'll\",\n",
    "               \"would\" , 'even' , 'could' , 'may'] \n",
    "stop_words.extend(my_stopwords)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "cleaned_reviews = []\n",
    "for review in mcd_review['review']:\n",
    "    tokens = tokenizer(review)\n",
    "    new_words = []\n",
    "    for token in tokens:\n",
    "        if (token.text in stop_words) == True:\n",
    "            continue\n",
    "        new_words.append(token.text)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    if len(tokenizer(cleaned_review)) < 3:\n",
    "        cleaned_review = \" \"\n",
    "    cleaned_reviews.append(cleaned_review)\n",
    "    \n",
    "mcd_review['clean'] = cleaned_reviews\n",
    "\n",
    "## Clean up the stop words and clean up the reviews with less than 3 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       (not, huge, macdonald, lover, better, ones, fa...\n",
       "1       (terrible, customer, service, came, 9:30pm, st...\n",
       "2       (first, \", lost, \", order, actually, gave, som...\n",
       "3       (see, not, one, giving, 1, star, not, -25, sta...\n",
       "4       (well, macdonald, know, food, review, reflects...\n",
       "                              ...                        \n",
       "1520    (enjoyed, part, repeatedly, asked, right, sauc...\n",
       "1521    (worst, macdonald, long, time, dirt, everywher...\n",
       "1522    (craving, macdonald, seems, closest, big, fan,...\n",
       "1523    (two, points, right, gate, :, 1, thuggery, kno...\n",
       "1524    (wanted, grab, breakfast, one, morning, work, ...\n",
       "Name: nlp_review, Length: 1525, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcd_review['nlp_review'] = mcd_review['clean'].apply(lambda x: nlp(x))\n",
    "mcd_review['nlp_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "pair = []\n",
    "for i in range(len(mcd_review['nlp_review'])):\n",
    "    max_simi = 0\n",
    "    for j in range((i+1),len(mcd_review['nlp_review'])):\n",
    "        cos_simi = mcd_review['nlp_review'][i].similarity(mcd_review['nlp_review'][j])\n",
    "        if (cos_simi > max_simi) & (cos_simi < 0.99):\n",
    "            max_simi = cos_simi\n",
    "            pair_max = [i,j]\n",
    "    result.append(max_simi)\n",
    "    pair.append(pair_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8621989217718908,\n",
       " 0.9115396436690267,\n",
       " 0.924136024456119,\n",
       " 0.6422144044566914,\n",
       " 0.932900777156394,\n",
       " 0.8759543947034448,\n",
       " 0.942150253737846,\n",
       " 0.8912123527406389,\n",
       " 0.9465620564368024,\n",
       " 0.938219050061854,\n",
       " 0.9422890407852071,\n",
       " 0.9281661050084306,\n",
       " 0.9568781601954796,\n",
       " 0.9246775964845924,\n",
       " 0.8544339774662885,\n",
       " 0.8052090624975229,\n",
       " 0.8652852520385924,\n",
       " 0.8526119849942638,\n",
       " 0.9325568521633459,\n",
       " 0.8558044071778608,\n",
       " 0.9140992574002214,\n",
       " 0.9526042283284065,\n",
       " 0.9399600361612748,\n",
       " 0.9028773111252866,\n",
       " 0.9255377935980866,\n",
       " 0.8602396502243306,\n",
       " 0.9147350808113301,\n",
       " 0.9476791284442903,\n",
       " 0.8169986334787053,\n",
       " 0.8777966768281186,\n",
       " 0.943202918333796,\n",
       " 0.7742351625770797,\n",
       " 0.9083546457360427,\n",
       " 0.9373132848965398,\n",
       " 0.9341253552659061,\n",
       " 0.8153806180878975,\n",
       " 0.8619325336343295,\n",
       " 0.8577022516166742,\n",
       " 0.8442683143949763,\n",
       " 0.9693800317857079,\n",
       " 0.8545105528284136,\n",
       " 0.9208283628578385,\n",
       " 0.8023870307527575,\n",
       " 0.9083646884946581,\n",
       " 0.848210641453852,\n",
       " 0.8868130274561492,\n",
       " 0.8331603668606252,\n",
       " 0.9236275902039974,\n",
       " 0.9303420041981367,\n",
       " 0.4076652117038546,\n",
       " 0.9250644319754722,\n",
       " 0.8862863521776626,\n",
       " 0.8894866639860596,\n",
       " 0.87255456363027,\n",
       " 0.9074692703385192,\n",
       " 0.9475602421495455,\n",
       " 0.8822489451105968,\n",
       " 0.9341406893152628,\n",
       " 0.9275750126217434,\n",
       " 0.9071152166484433,\n",
       " 0.9181422221111158,\n",
       " 0.952097557716517,\n",
       " 0.9179417915689877,\n",
       " 0.9436568727230822,\n",
       " 0.5494015455784538,\n",
       " 0.9518516738611311,\n",
       " 0.8812826114920639,\n",
       " 0.9040751188056925,\n",
       " 0.9502573315224104,\n",
       " 0.8167212723175087,\n",
       " 0.9097085132571301,\n",
       " 0.5775419098080364,\n",
       " 0.9223790812781782,\n",
       " 0.9222330436085888,\n",
       " 0.7886495446471774,\n",
       " 0.7752397497359916,\n",
       " 0.817885440780531,\n",
       " 0.9416345530645016,\n",
       " 0.8831437215525165,\n",
       " 0.826402264977831,\n",
       " 0.9017283924523378,\n",
       " 0.9494072634867297,\n",
       " 0.8605645667823683,\n",
       " 0.9122220820618056,\n",
       " 0.8987562313351999,\n",
       " 0.903924910956743,\n",
       " 0.9368307696733836,\n",
       " 0.9393328827025287,\n",
       " 0.9365652039685756,\n",
       " 0.9013735473832882,\n",
       " 0.9414444800689766,\n",
       " 0.915444741994689,\n",
       " 0.9423854648019425,\n",
       " 0.9451320501329794,\n",
       " 0.8718316851468625,\n",
       " 0.8969998951009845,\n",
       " 0.8710005101110349,\n",
       " 0.9305571242013436,\n",
       " 0.9224638772367026,\n",
       " 0.8823811833023855,\n",
       " 0.8277114398874154,\n",
       " 0.9038640475481036,\n",
       " 0.8960423221939803,\n",
       " 0.9048406787389983,\n",
       " 0.74860561029372,\n",
       " 0.8857093581866086,\n",
       " 0.8551353373746973,\n",
       " 0.8969731323594993,\n",
       " 0.9402030612437373,\n",
       " 0.7936708388975254,\n",
       " 0.6801092348153625,\n",
       " 0.8242230700235396,\n",
       " 0.9497426375434997,\n",
       " 0.9017198579678439,\n",
       " 0.9331843320149102,\n",
       " 0.8857507682261233,\n",
       " 0.934923665561984,\n",
       " 0.9348807316845392,\n",
       " 0.9567604971262736,\n",
       " 0.9578266032719099,\n",
       " 0.9136176925192322,\n",
       " 0.9207983767660083,\n",
       " 0.9469863802488364,\n",
       " 0.9323898684604415,\n",
       " 0.8385663624703947,\n",
       " 0.8937699932643074,\n",
       " 0.9300146379057654,\n",
       " 0.8275061854993617,\n",
       " 0.4076652117038546,\n",
       " 0.8856431371894438,\n",
       " 0.7860928792884139,\n",
       " 0.9322010461484095,\n",
       " 0.8858165773144409,\n",
       " 0.8910573468641331,\n",
       " 0.8681797432275086,\n",
       " 0.9025535636919886,\n",
       " 0.8261467134134592,\n",
       " 0.9308243917458302,\n",
       " 0.9073844359129672,\n",
       " 0.9217492306106874,\n",
       " 0.9243654869141842,\n",
       " 0.9153756673177308,\n",
       " 0.8958518972329179,\n",
       " 0.868787337781287,\n",
       " 0.9226487677376306,\n",
       " 0.9269595622877765,\n",
       " 0.7789873204259549,\n",
       " 0.9130016656741087,\n",
       " 0.7768421406052343,\n",
       " 0.9054370052706713,\n",
       " 0.9322301355358761,\n",
       " 0.9295673874158658,\n",
       " 0.8846402191872565,\n",
       " 0.9238151049862818,\n",
       " 0.8477665516580909,\n",
       " 0.8976689623322365,\n",
       " 0.876060958766698,\n",
       " 0.8660052768426632,\n",
       " 0.8744643713356394,\n",
       " 0.9218681255500385,\n",
       " 0.926179246507341,\n",
       " 0.7779155220562801,\n",
       " 0.8688890132050255,\n",
       " 0.8401987230498594,\n",
       " 0.8345479470305036,\n",
       " 0.9559719865620954,\n",
       " 0.949564228026827,\n",
       " 0.8962269094182785,\n",
       " 0.8832419913773056,\n",
       " 0.8236660860282612,\n",
       " 0.8617139823185789,\n",
       " 0.8491395337052966,\n",
       " 0.9105523629307639,\n",
       " 0.9352604935311308,\n",
       " 0.8654435251426688,\n",
       " 0.9239669402525357,\n",
       " 0.9194685218460055,\n",
       " 0.8320236287947266,\n",
       " 0.9355445264539034,\n",
       " 0.8668881809226724,\n",
       " 0.9657654405010825,\n",
       " 0.7938799456998654,\n",
       " 0.9332315923686588,\n",
       " 0.9528650319429787,\n",
       " 0.8052782618277544,\n",
       " 0.7861898746490859,\n",
       " 0.9442979966745997,\n",
       " 0.9617732418036612,\n",
       " 0.9086971190545626,\n",
       " 0.9105798197500199,\n",
       " 0.8471385280668124,\n",
       " 0.8460235463742306,\n",
       " 0.9458733600403726,\n",
       " 0.9088968018390896,\n",
       " 0.9470948480292248,\n",
       " 0.8753183551970163,\n",
       " 0.9176990900524356,\n",
       " 0.6909518934497311,\n",
       " 0.9325107084347135,\n",
       " 0.7893957052215407,\n",
       " 0.8902060056662271,\n",
       " 0.9494849811463625,\n",
       " 0.8582552364127694,\n",
       " 0.9184468480078976,\n",
       " 0.8839683198406808,\n",
       " 0.9248793052172003,\n",
       " 0.8985688978599269,\n",
       " 0.9192084611484719,\n",
       " 0.7758709876243168,\n",
       " 0.9185678493385633,\n",
       " 0.9359474526023435,\n",
       " 0.9172026093929672,\n",
       " 0.8729615934802873,\n",
       " 0.9182934310637951,\n",
       " 0.7762232918149639,\n",
       " 0.9083551986816093,\n",
       " 0.9158003530700572,\n",
       " 0.9190167382707379,\n",
       " 0.922274304708907,\n",
       " 0.8475491417616425,\n",
       " 0.9325281982734446,\n",
       " 0.936790573177993,\n",
       " 0.9362827651276924,\n",
       " 0.8978853823780345,\n",
       " 0.9257423023175841,\n",
       " 0.8918806691527521,\n",
       " 0.9328290538258732,\n",
       " 0.9185463535628006,\n",
       " 0.9152969506280983,\n",
       " 0.9549393672167532,\n",
       " 0.8463181562492719,\n",
       " 0.9106883457011399,\n",
       " 0.9273578177418329,\n",
       " 0.9335361618946979,\n",
       " 0.8867832106801528,\n",
       " 0.7986775485575167,\n",
       " 0.8758754631294579,\n",
       " 0.9478769231821975,\n",
       " 0.8523808896797115,\n",
       " 0.9428808419776101,\n",
       " 0.8806620056598751,\n",
       " 0.9339539561438605,\n",
       " 0.7881506181523178,\n",
       " 0.6919189976036276,\n",
       " 0.9676016210077856,\n",
       " 0.9307196511302175,\n",
       " 0.9369869460033883,\n",
       " 0.8889862739609509,\n",
       " 0.8779109286547124,\n",
       " 0.8754361442906777,\n",
       " 0.827870239323408,\n",
       " 0.8231123496863534,\n",
       " 0.9090305249259177,\n",
       " 0.849547213483567,\n",
       " 0.8753307340165434,\n",
       " 0.8249423206597716,\n",
       " 0.9273230922744845,\n",
       " 0.8962812564405619,\n",
       " 0.9062019764473986,\n",
       " 0.8741912757535173,\n",
       " 0.9407504056739185,\n",
       " 0.9480033553090726,\n",
       " 0.9109244067651944,\n",
       " 0.954265211039224,\n",
       " 0.922468342946178,\n",
       " 0.9175681587174607,\n",
       " 0.7862041173926808,\n",
       " 0.9282608226542945,\n",
       " 0.6295049098108229,\n",
       " 0.8425036867769256,\n",
       " 0.7396590386325055,\n",
       " 0.8887163633385575,\n",
       " 0.885374228677017,\n",
       " 0.9396825437681942,\n",
       " 0.9288039092628042,\n",
       " 0.9188110116161158,\n",
       " 0.9211153921300274,\n",
       " 0.8781187486255089,\n",
       " 0.800263395798054,\n",
       " 0.9173835881018563,\n",
       " 0.9467495618025538,\n",
       " 0.8335590509013371,\n",
       " 0.9421952907410724,\n",
       " 0.8931151364048128,\n",
       " 0.8819851556123389,\n",
       " 0.878730557281624,\n",
       " 0.9410631170421755,\n",
       " 0.9457393971963478,\n",
       " 0.8454562266185238,\n",
       " 0.9104895828018237,\n",
       " 0.9553252006972384,\n",
       " 0.9179263741573648,\n",
       " 0.8709573093861476,\n",
       " 0.9134767158222923,\n",
       " 0.9333060160399089,\n",
       " 0.8688544024725386,\n",
       " 0.9409704621468246,\n",
       " 0.9458609569104719,\n",
       " 0.8935050142565429,\n",
       " 0.9248106493246945,\n",
       " 0.8708511467514891,\n",
       " 0.9579767374205264,\n",
       " 0.9030456460539948,\n",
       " 0.9531763527167217,\n",
       " 0.9217706843012604,\n",
       " 0.9570333379662145,\n",
       " 0.929392568065323,\n",
       " 0.9517931565429089,\n",
       " 0.9380447910742248,\n",
       " 0.9358663188034939,\n",
       " 0.9277753657426907,\n",
       " 0.9425814281963573,\n",
       " 0.9202289884759853,\n",
       " 0.9222196395262237,\n",
       " 0.9224729876672607,\n",
       " 0.9223803345105354,\n",
       " 0.9317644735145787,\n",
       " 0.9185456814612857,\n",
       " 0.7817176341221295,\n",
       " 0.8234058811613089,\n",
       " 0.7133543503712998,\n",
       " 0.9336304659414114,\n",
       " 0.8790360684954257,\n",
       " 0.920785571729327,\n",
       " 0.9443545951577369,\n",
       " 0.9300826568492842,\n",
       " 0.9471703863358388,\n",
       " 0.9467656938978821,\n",
       " 0.9177711849792637,\n",
       " 0.8867046016706659,\n",
       " 0.9250064219131159,\n",
       " 0.8831829578411481,\n",
       " 0.8339128575450092,\n",
       " 0.9506501047988495,\n",
       " 0.8667858204275088,\n",
       " 0.925061228864672,\n",
       " 0.9290588003572587,\n",
       " 0.8207209829231036,\n",
       " 0.9016683874088204,\n",
       " 0.9315437794178841,\n",
       " 0.9293963174556179,\n",
       " 0.7402758552411284,\n",
       " 0.8949673372945779,\n",
       " 0.8875047336246039,\n",
       " 0.917670682981557,\n",
       " 0.8622155826872299,\n",
       " 0.9229514170568198,\n",
       " 0.9511524637908504,\n",
       " 0.902048980098398,\n",
       " 0.9209793609640192,\n",
       " 0.9080526682118985,\n",
       " 0.9369862708628237,\n",
       " 0.8315957045020644,\n",
       " 0.8903546313728211,\n",
       " 0.9171371070204474,\n",
       " 0.9465297410092193,\n",
       " 0.8729421627996184,\n",
       " 0.8604200896457598,\n",
       " 0.9187354936405187,\n",
       " 0.8291577255577968,\n",
       " 0.8579192998044014,\n",
       " 0.8738662733528526,\n",
       " 0.9281974535590755,\n",
       " 0.8747270937261356,\n",
       " 0.9546093791976358,\n",
       " 0.8357941491605706,\n",
       " 0.9141032156139713,\n",
       " 0.49265153652121846,\n",
       " 0.9043464044352729,\n",
       " 0.9496517121568321,\n",
       " 0.854490712933691,\n",
       " 0.868101380800186,\n",
       " 0.9010411238464254,\n",
       " 0.9586417112524201,\n",
       " 0.8363093804340762,\n",
       " 0.898752456880512,\n",
       " 0.9375982759111443,\n",
       " 0.9386909861997025,\n",
       " 0.9328917537152132,\n",
       " 0.8584971103552776,\n",
       " 0.9172446123876081,\n",
       " 0.816858667657208,\n",
       " 0.862484581894633,\n",
       " 0.9512753832755005,\n",
       " 0.7830328388826897,\n",
       " 0.7986983635886313,\n",
       " 0.890722421550012,\n",
       " 0.7799404430302131,\n",
       " 0.943375385878943,\n",
       " 0.9377371358422484,\n",
       " 0.9264587501807425,\n",
       " 0.8195078296954754,\n",
       " 0.9227504457344983,\n",
       " 0.8917540762380575,\n",
       " 0.9120604814088512,\n",
       " 0.9175932407941961,\n",
       " 0.8772565600346871,\n",
       " 0.9058760712777671,\n",
       " 0.9491189965413335,\n",
       " 0.9651896237899771,\n",
       " 0.911489321874885,\n",
       " 0.9443344488584975,\n",
       " 0.8505941180211963,\n",
       " 0.9155919357729227,\n",
       " 0.9097680327888784,\n",
       " 0.8691005534070073,\n",
       " 0.9369476496862343,\n",
       " 0.8641673351405925,\n",
       " 0.9649501485465319,\n",
       " 0.8873697742067065,\n",
       " 0.948115239116708,\n",
       " 0.923572349029721,\n",
       " 0.6764732984294684,\n",
       " 0.8660750509931296,\n",
       " 0.9472869673415899,\n",
       " 0.9194287677213904,\n",
       " 0.940026866468943,\n",
       " 0.9246310210898377,\n",
       " 0.8512773933148585,\n",
       " 0.8696706934085312,\n",
       " 0.9614648297557049,\n",
       " 0.8788532231166848,\n",
       " 0.9076890011352483,\n",
       " 0.8286864352925778,\n",
       " 0.9301722950812527,\n",
       " 0.9061981856727183,\n",
       " 0.9059657823448031,\n",
       " 0.9333607662312848,\n",
       " 0.8072758101512281,\n",
       " 0.8955674834507745,\n",
       " 0.8962578265890424,\n",
       " 0.9395659998980306,\n",
       " 0.8505714210715432,\n",
       " 0.7169115917011727,\n",
       " 0.8771092049755967,\n",
       " 0.8507369916061803,\n",
       " 0.8916425081592954,\n",
       " 0.9149441002097433,\n",
       " 0.9448200499644089,\n",
       " 0.8743622042160154,\n",
       " 0.9007265638450643,\n",
       " 0.8443552972145321,\n",
       " 0.9023791303507489,\n",
       " 0.9059005363067262,\n",
       " 0.8855411996847302,\n",
       " 0.9501166857279748,\n",
       " 0.6923283570673997,\n",
       " 0.897746291854009,\n",
       " 0.9609108629193464,\n",
       " 0.926904951020527,\n",
       " 0.9062999932994921,\n",
       " 0.916264258882598,\n",
       " 0.5973856416857257,\n",
       " 0.9007362587566302,\n",
       " 0.9194120543223335,\n",
       " 0.9173969387180513,\n",
       " 0.9120026008427106,\n",
       " 0.8919347364954606,\n",
       " 0.8974642498998046,\n",
       " 0.9402292288941855,\n",
       " 0.937320721543578,\n",
       " 0.8699327422474711,\n",
       " 0.8892193570735268,\n",
       " 0.9309412424954362,\n",
       " 0.8230335917919805,\n",
       " 0.9185490171070593,\n",
       " 0.8705124391206929,\n",
       " 0.8171860981414588,\n",
       " 0.8640018210020979,\n",
       " 0.8421468577218518,\n",
       " 0.903680274408963,\n",
       " 0.9312526713153246,\n",
       " 0.876462255754061,\n",
       " 0.8910333920292857,\n",
       " 0.9571569497460435,\n",
       " 0.9619968488010198,\n",
       " 0.866484353317217,\n",
       " 0.884041771662142,\n",
       " 0.9390187994503616,\n",
       " 0.9295023657242043,\n",
       " 0.9489186164639717,\n",
       " 0.8938918294934806,\n",
       " 0.9361327521186495,\n",
       " 0.9388668979397835,\n",
       " 0.9066973622843675,\n",
       " 0.9152656022063212,\n",
       " 0.9340278465241055,\n",
       " 0.9066569978042679,\n",
       " 0.8170577615221142,\n",
       " 0.935210474905998,\n",
       " 0.9290931331285267,\n",
       " 0.8610024178712171,\n",
       " 0.9409500040071054,\n",
       " 0.8705411566413963,\n",
       " 0.8677148258150733,\n",
       " 0.9276305805552826,\n",
       " 0.9318856233412148,\n",
       " 0.8848979890463856,\n",
       " 0.7379810745798591,\n",
       " 0.9244293197268552,\n",
       " 0.8110893868248317,\n",
       " 0.9308104854615773,\n",
       " 0.9429841943123008,\n",
       " 0.9046167669885264,\n",
       " 0.8308382834763172,\n",
       " 0.760886787580705,\n",
       " 0.9635844940159575,\n",
       " 0.943450286992044,\n",
       " 0.8882537965780818,\n",
       " 0.9071378849349071,\n",
       " 0.9205119140414013,\n",
       " 0.9165409987451651,\n",
       " 0.7866663261867348,\n",
       " 0.9552446818088665,\n",
       " 0.9383306518794425,\n",
       " 0.8645249354683227,\n",
       " 0.9051918613216713,\n",
       " 0.9455082536001375,\n",
       " 0.7085786859473949,\n",
       " 0.8364473035308143,\n",
       " 0.8464514994948176,\n",
       " 0.912521134371418,\n",
       " 0.7692940649707308,\n",
       " 0.8519490093273593,\n",
       " 0.9546966181567659,\n",
       " 0.9122816196328017,\n",
       " 0.9019176852087015,\n",
       " 0.8825220094280122,\n",
       " 0.9470689142022469,\n",
       " 0.8589376386613496,\n",
       " 0.9419872886396651,\n",
       " 0.8511689133092508,\n",
       " 0.9124350295471223,\n",
       " 0.9528814058864425,\n",
       " 0.8481419222957378,\n",
       " 0.9309548893144786,\n",
       " 0.9109477297656375,\n",
       " 0.8570438632749284,\n",
       " 0.8966030031676862,\n",
       " 0.9615959231251576,\n",
       " 0.8651347423031412,\n",
       " 0.9446682821371833,\n",
       " 0.9105436451180888,\n",
       " 0.951996069157964,\n",
       " 0.9202766237055973,\n",
       " 0.7908195321520657,\n",
       " 0.8460796093235192,\n",
       " 0.9459869796145397,\n",
       " 0.9505422001538613,\n",
       " 0.9174397417640722,\n",
       " 0.9425401352560713,\n",
       " 0.8690107459661265,\n",
       " 0.8297504190519569,\n",
       " 0.8629639322007472,\n",
       " 0.8655079485077853,\n",
       " 0.8543411497800533,\n",
       " 0.9422825868932347,\n",
       " 0.9012071458432428,\n",
       " 0.8276058011622567,\n",
       " 0.8011554454309191,\n",
       " 0.9236232939049437,\n",
       " 0.8746640877596596,\n",
       " 0.9203687211895966,\n",
       " 0.9266767244498326,\n",
       " 0.9209418957506852,\n",
       " 0.9478685438982267,\n",
       " 0.8893872307846853,\n",
       " 0.9122060132796788,\n",
       " 0.9141524921146346,\n",
       " 0.855421269088436,\n",
       " 0.9268039791296723,\n",
       " 0.9283320187501835,\n",
       " 0.8272085876465408,\n",
       " 0.8963482883347862,\n",
       " 0.9197235739593251,\n",
       " 0.8647308397777853,\n",
       " 0.9474586528761206,\n",
       " 0.9396196536514438,\n",
       " 0.8963687368319213,\n",
       " 0.8171894952324021,\n",
       " 0.9282158376581051,\n",
       " 0.8672552208157163,\n",
       " 0.912463912213431,\n",
       " 0.8999764661352295,\n",
       " 0.922068995660602,\n",
       " 0.8979340775051736,\n",
       " 0.8481892625881188,\n",
       " 0.8387845338035853,\n",
       " 0.9502614186367674,\n",
       " 0.9472549490685326,\n",
       " 0.9080231077373666,\n",
       " 0.8878400854962442,\n",
       " 0.9623521261427248,\n",
       " 0.9569988423190485,\n",
       " 0.889914342234751,\n",
       " 0.8956204828853944,\n",
       " 0.8998777111796862,\n",
       " 0.8996497259985582,\n",
       " 0.8829717768138792,\n",
       " 0.9312506612978212,\n",
       " 0.8979501855797791,\n",
       " 0.9207639143950812,\n",
       " 0.9343353427949735,\n",
       " 0.8951980124006234,\n",
       " 0.9134093398012334,\n",
       " 0.8714740592895126,\n",
       " 0.9554005541093222,\n",
       " 0.9255857745429605,\n",
       " 0.8121158309476714,\n",
       " 0.931857338271189,\n",
       " 0.8483413935151243,\n",
       " 0.9456557280992477,\n",
       " 0.9353243969377226,\n",
       " 0.9659713212904742,\n",
       " 0.9187797560040813,\n",
       " 0.7718681516716696,\n",
       " 0.922158243705025,\n",
       " 0.9450224254945921,\n",
       " 0.9444513269488453,\n",
       " 0.900460800643567,\n",
       " 0.8690870346168961,\n",
       " 0.9041952295923116,\n",
       " 0.9138304950419817,\n",
       " 0.8438485821793968,\n",
       " 0.7452156240767425,\n",
       " 0.9214561987446046,\n",
       " 0.8210401895047663,\n",
       " 0.8833194801337868,\n",
       " 0.8608828161284958,\n",
       " 0.7212098187446828,\n",
       " 0.9388496406561325,\n",
       " 0.9448769958571201,\n",
       " 0.8612737486158674,\n",
       " 0.8519700304031596,\n",
       " 0.9721677563055974,\n",
       " 0.7542163004358112,\n",
       " 0.8618118662875195,\n",
       " 0.7199595037457897,\n",
       " 0.911315271732972,\n",
       " 0.9054885920657137,\n",
       " 0.9129595322023596,\n",
       " 0.8018380939384142,\n",
       " 0.8773339621640426,\n",
       " 0.4076652117038546,\n",
       " 0.7726032400281622,\n",
       " 0.8732170973469229,\n",
       " 0.8856623948051174,\n",
       " 0.8907446277440929,\n",
       " 0.8727345699216608,\n",
       " 0.9216042093369211,\n",
       " 0.9258240856513917,\n",
       " 0.9497967886210565,\n",
       " 0.7142366905698005,\n",
       " 0.8562573990280056,\n",
       " 0.9061953100813607,\n",
       " 0.8718833519459759,\n",
       " 0.9138865464654017,\n",
       " 0.9358603642902716,\n",
       " 0.9269278350986337,\n",
       " 0.8541225576313518,\n",
       " 0.8969449314766778,\n",
       " 0.889500582212524,\n",
       " 0.898556641277047,\n",
       " 0.934678781939538,\n",
       " 0.8043578802821665,\n",
       " 0.9219539782702915,\n",
       " 0.9541972875920306,\n",
       " 0.9344032571968593,\n",
       " 0.7731575712177227,\n",
       " 0.9177996648374582,\n",
       " 0.6026363660736722,\n",
       " 0.8292481616576213,\n",
       " 0.8251117170021444,\n",
       " 0.9461405300588261,\n",
       " 0.9442138576271764,\n",
       " 0.7177280999359251,\n",
       " 0.4076652117038546,\n",
       " 0.8403494038530468,\n",
       " 0.8865070302228066,\n",
       " 0.8775884441203188,\n",
       " 0.827363429247948,\n",
       " 0.8759757460423998,\n",
       " 0.966340293760599,\n",
       " 0.9521353822062483,\n",
       " 0.9309901873979751,\n",
       " 0.8603995988485301,\n",
       " 0.9181999346569515,\n",
       " 0.9279779636026336,\n",
       " 0.861206615876911,\n",
       " 0.8918323774982638,\n",
       " 0.9093706790255761,\n",
       " 0.9180181924657794,\n",
       " 0.9354985473366937,\n",
       " 0.9124783943392509,\n",
       " 0.849408890088207,\n",
       " 0.9468425310598051,\n",
       " 0.943385793238214,\n",
       " 0.8512215871929637,\n",
       " 0.9107362828705372,\n",
       " 0.8779899181700993,\n",
       " 0.8688054711298416,\n",
       " 0.8976749439302356,\n",
       " 0.7074680228452962,\n",
       " 0.9080887627425803,\n",
       " 0.9457417511104446,\n",
       " 0.9483137153372195,\n",
       " 0.744953033234495,\n",
       " 0.7634242222908596,\n",
       " 0.9587793741190868,\n",
       " 0.9325577888231803,\n",
       " 0.9371299821732032,\n",
       " 0.912122442019776,\n",
       " 0.8975366835200557,\n",
       " 0.9590044966018605,\n",
       " 0.8249550103823465,\n",
       " 0.9247980836919373,\n",
       " 0.8997072898176364,\n",
       " 0.8623294895466901,\n",
       " 0.9588549139001629,\n",
       " 0.9074887327042355,\n",
       " 0.9328331518875447,\n",
       " 0.9351066545623894,\n",
       " 0.862989458279924,\n",
       " 0.9223199388896635,\n",
       " 0.901775975547534,\n",
       " 0.9128229680199569,\n",
       " 0.9269842822342143,\n",
       " 0.7744420179938565,\n",
       " 0.9290414565681799,\n",
       " 0.818145839344675,\n",
       " 0.9139903851007,\n",
       " 0.9478766283639308,\n",
       " 0.9572647212632397,\n",
       " 0.912485570193065,\n",
       " 0.8714272818265987,\n",
       " 0.9084202638970398,\n",
       " 0.967048227387115,\n",
       " 0.8982425560551248,\n",
       " 0.8044541334364732,\n",
       " 0.9405913464293422,\n",
       " 0.8588256618706868,\n",
       " 0.9369457115522424,\n",
       " 0.8983264749498701,\n",
       " 0.8830483173308904,\n",
       " 0.8884386364545921,\n",
       " 0.9390199118936127,\n",
       " 0.8713321439011854,\n",
       " 0.9154778802811167,\n",
       " 0.9413824312763137,\n",
       " 0.8052233344293227,\n",
       " 0.937172945704498,\n",
       " 0.7606688007217935,\n",
       " 0.9484009797720776,\n",
       " 0.944529516223855,\n",
       " 0.9348627957140795,\n",
       " 0.8474605444234861,\n",
       " 0.8680023660905984,\n",
       " 0.9197041921807524,\n",
       " 0.9444414665096815,\n",
       " 0.8587001433283923,\n",
       " 0.7948665575288978,\n",
       " 0.9554505946155831,\n",
       " 0.8651164599925462,\n",
       " 0.869062974624215,\n",
       " 0.7912223173722314,\n",
       " 0.8835039918504659,\n",
       " 0.9497877318797975,\n",
       " 0.9283555067845194,\n",
       " 0.8846435356623609,\n",
       " 0.9409580832884382,\n",
       " 0.9677330461426117,\n",
       " 0.9103121077928886,\n",
       " 0.8650920093461967,\n",
       " 0.9159236035381041,\n",
       " 0.6896840660629211,\n",
       " 0.9400761424862456,\n",
       " 0.8373575895098255,\n",
       " 0.9210377206225613,\n",
       " 0.9035632481586013,\n",
       " 0.8618266230765848,\n",
       " 0.950877016141623,\n",
       " 0.8134872914567879,\n",
       " 0.9301493054304555,\n",
       " 0.8355537849705926,\n",
       " 0.8882739870422466,\n",
       " 0.9481637493902619,\n",
       " 0.8075693869547215,\n",
       " 0.9265058877504547,\n",
       " 0.8760351271865311,\n",
       " 0.9185461627349903,\n",
       " 0.9311779384268287,\n",
       " 0.9135358759360822,\n",
       " 0.8843486851429151,\n",
       " 0.9273474719351873,\n",
       " 0.820061004698201,\n",
       " 0.9162737868211216,\n",
       " 0.8846326736044535,\n",
       " 0.9574192906282364,\n",
       " 0.751461583025559,\n",
       " 0.9414480902248462,\n",
       " 0.8639301523406778,\n",
       " 0.8907919581380602,\n",
       " 0.884592762738759,\n",
       " 0.8463674837796535,\n",
       " 0.9053027632123419,\n",
       " 0.9428320817223699,\n",
       " 0.9189059480286145,\n",
       " 0.8840305922435621,\n",
       " 0.9470566172617064,\n",
       " 0.8327061375210674,\n",
       " 0.8872068632354178,\n",
       " 0.8773823573517278,\n",
       " 0.8926597937772622,\n",
       " 0.9126628547894452,\n",
       " 0.8881321922707507,\n",
       " 0.89225582742054,\n",
       " 0.8955155127689473,\n",
       " 0.721635640341523,\n",
       " 0.9296278149704649,\n",
       " 0.9250137277292825,\n",
       " 0.9039498739572529,\n",
       " 0.8116966283828084,\n",
       " 0.9028230172302816,\n",
       " 0.9311278486971858,\n",
       " 0.9087590650470714,\n",
       " 0.9206082462281413,\n",
       " 0.5843350833090156,\n",
       " 0.9002857477376622,\n",
       " 0.6291121014276756,\n",
       " 0.9478205614536165,\n",
       " 0.8195047966768336,\n",
       " 0.844729220557996,\n",
       " 0.9293059626726425,\n",
       " 0.8788846392727048,\n",
       " 0.8817785300825932,\n",
       " 0.930536351278115,\n",
       " 0.9485457015428225,\n",
       " 0.918194808820175,\n",
       " 0.813396839659015,\n",
       " 0.9071820463821952,\n",
       " 0.9574789192725239,\n",
       " 0.7026738314305461,\n",
       " 0.8576750599364718,\n",
       " 0.8912427009359268,\n",
       " 0.9731006464516134,\n",
       " 0.9648134302083466,\n",
       " 0.8067188899645849,\n",
       " 0.9318317154129471,\n",
       " 0.9284454455165503,\n",
       " 0.9176595551494774,\n",
       " 0.9186522803001912,\n",
       " 0.9251793855973256,\n",
       " 0.7079131986774798,\n",
       " 0.9526022576053876,\n",
       " 0.9138144744553425,\n",
       " 0.9158025622673986,\n",
       " 0.8770200442098797,\n",
       " 0.894188515478922,\n",
       " 0.8660647103840141,\n",
       " 0.9042449407954991,\n",
       " 0.9144739922644237,\n",
       " 0.9211000756289843,\n",
       " 0.9573571388207437,\n",
       " 0.9382129320195233,\n",
       " 0.9162673507831525,\n",
       " 0.8312066052328785,\n",
       " 0.8926330180608746,\n",
       " 0.9262552800639738,\n",
       " 0.9233407677423199,\n",
       " 0.8896181240189859,\n",
       " 0.8675332222058603,\n",
       " 0.9389030165981586,\n",
       " 0.8759583564154216,\n",
       " 0.8813132889132549,\n",
       " 0.9128695600596173,\n",
       " 0.9347233865752778,\n",
       " 0.7851634676744607,\n",
       " 0.8887117968592119,\n",
       " 0.8760291901481146,\n",
       " 0.9217125686511517,\n",
       " 0.8506427798503637,\n",
       " 0.9128251262581906,\n",
       " 0.9190190886113826,\n",
       " 0.8523614501293764,\n",
       " 0.8959958516987019,\n",
       " 0.8884066800863124,\n",
       " 0.8030021781736835,\n",
       " 0.9585731624263458,\n",
       " 0.8355258684414144,\n",
       " 0.9223571906434228,\n",
       " 0.9066581745061585,\n",
       " 0.8580498858375625,\n",
       " 0.9310490604041116,\n",
       " 0.9373576624051071,\n",
       " 0.9231216249257429,\n",
       " 0.9143348154118244,\n",
       " 0.8874670854842115,\n",
       " 0.8090900758345659,\n",
       " 0.8343450489956284,\n",
       " 0.9338883392440588,\n",
       " 0.8800802681927617,\n",
       " 0.8881185367124719,\n",
       " 0.9288553675257929,\n",
       " 0.8879336782848215,\n",
       " 0.9213283592041117,\n",
       " 0.9226933696428463,\n",
       " 0.8395165963935362,\n",
       " 0.8946147916645136,\n",
       " 0.883894530470558,\n",
       " 0.7834553272736106,\n",
       " 0.9489814871218386,\n",
       " 0.5851785899858202,\n",
       " 0.8911173109283712,\n",
       " 0.9368158150280741,\n",
       " 0.9385400269636056,\n",
       " 0.8736679609805866,\n",
       " 0.9127019559427408,\n",
       " 0.6859967048846972,\n",
       " 0.9060737147044349,\n",
       " 0.8109835844319819,\n",
       " 0.8596954685552671,\n",
       " 0.9031774987514414,\n",
       " 0.8214518151090655,\n",
       " 0.9494777937167455,\n",
       " 0.9432187172223787,\n",
       " 0.9494777937167455,\n",
       " 0.9095073166527264,\n",
       " 0.9142556814433651,\n",
       " 0.7259059364316913,\n",
       " 0.9432187172223787,\n",
       " 0.4076652117038546,\n",
       " 0.9018720218014926,\n",
       " 0.9103936718561592,\n",
       " 0.9140751285890791,\n",
       " 0.9118947803316866,\n",
       " 0.8800368193963167,\n",
       " 0.9509085510767294,\n",
       " 0.8461023270551965,\n",
       " 0.9292796925199004,\n",
       " 0.7773123209804306,\n",
       " 0.915012605133923,\n",
       " 0.8779358798085614,\n",
       " 0.9193903110100871,\n",
       " 0.8230381244169318,\n",
       " 0.9042075663862639,\n",
       " 0.8837802688310119,\n",
       " 0.9382903535622351,\n",
       " 0.914576716480962,\n",
       " 0.9069705307305082,\n",
       " 0.8801334250182644,\n",
       " 0.8650376603140352,\n",
       " 0.870448682183039,\n",
       " 0.8118633290300021,\n",
       " 0.8696781321865039,\n",
       " 0.9527993579739612,\n",
       " 0.8650376603140352,\n",
       " 0.870448682183039,\n",
       " 0.8118633290300021,\n",
       " 0.8696781321865039,\n",
       " 0.9527993579739612,\n",
       " 0.7029717025992963,\n",
       " 0.9088239292318945,\n",
       " 0.8544643819333111,\n",
       " 0.8291088662092595,\n",
       " 0.8994362844950182,\n",
       " 0.8845703009780725,\n",
       " 0.8636257336128282,\n",
       " 0.861068464126757,\n",
       " 0.9462239338166049,\n",
       " 0.9191574017200842,\n",
       " 0.9508090710365731,\n",
       " 0.9383143888907953,\n",
       " 0.931980735121522,\n",
       " 0.9241225471560242,\n",
       " 0.8760984554193935,\n",
       " 0.9102699944536843,\n",
       " 0.8553410581186787,\n",
       " 0.8817389802196623,\n",
       " 0.9298482757316339,\n",
       " 0.9075452538123071,\n",
       " 0.9127539177230133,\n",
       " 0.7841696337337791,\n",
       " 0.879786755645459,\n",
       " 0.7936814939259003,\n",
       " 0.7949616947624051,\n",
       " 0.7988694010218397,\n",
       " 0.9294661431143459,\n",
       " 0.861967401343809,\n",
       " 0.9608392504121616,\n",
       " 0.7623707550636236,\n",
       " 0.9443603228407257,\n",
       " 0.8467309389747404,\n",
       " 0.9560718106428636,\n",
       " 0.8957794474113054,\n",
       " 0.8497823962867959,\n",
       " 0.9360334873343057,\n",
       " 0.8523651695395696,\n",
       " 0.9229210225860857,\n",
       " 0.9506280940177896,\n",
       " 0.8456650545927729,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 39, 634, 844])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_simi = np.argsort(result)[-3:]\n",
    "index_simi   \n",
    "## I use a code from stackoverflow to get the index for the 10 largest cosine similarity result. \n",
    "## https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "## I will use index to match up at last. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 187]\n",
      "[634, 1044]\n",
      "[844, 1385]\n"
     ]
    }
   ],
   "source": [
    "for each in index_simi.tolist():\n",
    "    print(pair[each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simiarlity Score 0.9693800317857079 \n",
      " Review 1: \n",
      " toxic dump in food quality and employee humanity/work efforttypically i do not eat macdonald food; yet similar to the experience of others \"trapped\" at northside hospital there is no other option if you are hungry outside of the hospital cafeteria hours of operation due to a relatives stay and my visits/overnights for 11 weeks~ i too found myself \"trapped\" and therefore forced to buy at macdonald mostly my visits were for coffee or ice cream sundaes however last night for the first time i ordered food (grilled chicken salad) i returned it ~ the meat smelled badly this did not surprise me and only enforced my resolve to avoid chain food restaurantswhat did surprise me and absolutely shocked me was the customer service offered by employees ~ how in the world can you be an employee here and not for one minute or even a second acknowledge the obvious fact that you work inside a hospital; then consider what that means for each and every person that approaches your counter to order how you do not see them in fact that you easily dismiss them is beyond my comprehension to understand how you lost your sense of humanity in all honestly should cause you to pause and contemplate this ~ are you not aware that you are reflecting how you measure within you your own sense of dignity?within my and family/friends of patient at northside hospital for the past 11 weeks: each and every time i/they had to place an order i /they was/were subjected to macdonald toxic business model (in food) which now has transferred (we learned) into their employees not once was i thanked for giving my order; not once was i thanked when delivered my order yet at every visit i waited and witnessed employee behaviors that were readily displayed and relayed in body language effort toward actual work and obvious attitude not only toward me but guests in front of me god forbid i happened to have question in reference to the menu (employee response: blank stare shrug looking behind them at menu only to turn back around with a blank stare and/or shrug again) god forbid on separate occasions i happened to ask for a spoon/napkin/straw last night i happened to ask for my drink please (after receiving my salad and waiting for longer than normal) yikes you think i had asked him to draw blood***one more detail regardless if i arrive at 2am 4am or between 7:30 pm ~ 11 pm this macdonald is always consistently filthy not just dirty but filthycan anyone lift a broom to sweep up the garbage in the service/cooking area; also in the dining room?can anyone clean the crud surrounding the plastic display case for cookies already? it's been 11 weeks and that crud is still smeared dried; just grosswash/disinfect the countertops payment pads straw holders everywhere spills occur; including areas customers touch every hour get it? every single hour get a cleaning only employee this employee will do nothing but be the ocd employee you need just stop this atrociously dirty & disgusting feeding trough you call a restaurant you should be ashamed**************************************************Ã®_**************************************northside hospital ~ you are responsible for allowing this restaurant to wallow within your walls how many more years will you ignore not only the unhealthy food options macdonald supplies~ but to continue to allow a daily dose of deliberate and diligent unhealthy negative and \"put upon\" attitudes from employees to be delivered to family friends and employees that are there only because they care and are there for your patients how this does not cause you to react and act is beyond mein my world with so many options available this place would be gone ~ like today; certainly not any longer than tomorrowthis toxic waste site is dumping daily doses of negativity inhumanity~ its seeping and reverberating within the walls of this hospital i know you feel it \n",
      " Review 2:  \n",
      " years of pathetic service disgusting food and dangerous mistakes why do i keep going back?having nothing to do with macdonald food in general and more to do with the ability of this specific location to produce it with out killing anybody i have been going to this particular store for years it is right around the corner from where i used to live and is on the way from where i live to pretty much everything i use (gas station costco movie theater market etc) it is the only convenient location and for a while macdonald was the only place that had $1 drinks so those are my lame excuses for repeatedly torturing myself with this horrible place service: i used to be a manager with another fast food chain i know what service should be i also know what it usually is it doesn't have to be 5 star but the rude attitudes ridiculously slow service and inability to get a single order right is really a bit too much when i say slow service i am talking 15 minutes is average drive thru time for a dollar menu burger and a soda correct orders: yes my order can occasionally be complicated (no castup add big mac sauce) but when i thought they could do no worse they messed up an order for 4 large drinks 2 diet cokes a lemonade and an unsweet iced tea (all with extra ice) we received a regular coke a lemonade a dr pepper and a sweet iced tea 2 drinks had no ice at all and the other 2 had a few pieces floating around we have never received a correct order from this store ever i could go on and on about it but i will stick with this sad example and move ondangerous/disgusting/incorrectly made: something a little different then an order where they put the wrong thing in the bag or mess up a custom is when they just make it incorrectly or dangerously some of the examples of this include pies that expired an hour before i received them an egg that was completely raw in the middle and tonights lovely mess up tonight i ordered the angus mushroom and swiss according to the website this should include 2 full slices of swiss cheese and a decent amount of mushrooms i had about a teaspoon of mushrooms no sauce and 1/2 a slice of cheese that is right they left off a slice and a half of cheese in addition it was a regular (and relatively old) quarter pound patty not the big ole angus patty they should have used i have called the number on the receipt multiple times that takes me to the district office they consistently send out a free burger or drink coupon to apologize they tell me how sorry they are and suggest i try one of their other locations as this is the only one they receive complaints from from what i was told they are revamping this store but i have been told that for well over a year and it seems to have been unvamped and going down hill which i didn't think was even possible i keep vowing this will be my last visit but then find myself sitting in the drive thru line yet again wondering if i will ever receive my food i highly do not recommend this location hopefully you won't be lured in by whatever the heck keeps luring me -sarah (this was written by paul's wife because i am usually the one dealing with them i guess he got smarter) \n",
      "\n",
      "\n",
      "Simiarlity Score 0.9721677563055974 \n",
      " Review 1: \n",
      " i'm a broke college student so macdonald's is always an option entering the establishment i was taken back at the cleanliness of the restaurant there was no line for me to order my food which is always convenient staff were pretty friendly and well groomed one guy even had a bible in his hand the employee that attended me actually had a warm smile on her face; this is unexpected to say the least food was hot and had some flavor of substance quality of food however is up to par with taco bell and other places that serve recycled dirty diapers in short i had an okay experience i actually felt like i was in a macdonald commercial i recommend the \"macdonald\" a double burger and macdonald smothered within two buns \n",
      " Review 2:  \n",
      " i travel quite a bit for the job and i hate to admit it but i eat at macdonald pretty frequently the healthy selections are actually pretty tasty and i love quarter pounders just hold the cheesemacdonald are generally consistent i find most of them are clean and professionally run  this one is an exception i stopped here for a macdonald and coffee before my flight i had a 10:00 am flight well past the morning rush the cooking areas of the restaurant were still dirty and the service was terrible they had a radio blaring some type of music and personal bags were hanging near the food prep areas the four people working were having trouble handling the two people in line an employee in uniform (off-duty i guess) actually cut in front of me when i was in line to get her meal  i complained but it was the \"manager\" who handed her the meal unreal i sent a very long email to macdonald you naturally set the bar lower for fast food restaurants but this macdonald managed to limbo under with plenty of room to spare 1 star because the macdonald was ok \n",
      "\n",
      "\n",
      "Simiarlity Score 0.9731006464516134 \n",
      " Review 1: \n",
      " if i were to die and go to hell this macdonald would certainly provide a worthy backdrop for a nightmare imaginedlet's begin the story of my worst dinner of my life at this sad little stop in a strip mall of no particular visual appeal under a bleak and cold chicago evening all things being fair the culinary experience was doomed for failure given certain personal and tragic circumstances i won't get into hereupon stepping to the counter i gave pause to the abbreviated menu not worthy of a macdonald express which is a new concept to me all-together i decided to play it safe with the classic burger meal with an intimidating amount of fries and drink aka \"the value meal\" my meal was served up a little too quickly with all due credit to the server the food seemed to appear from nowhere - gremlins cooking the food? i could see the grease seeping through one of the burger wrappers at this point i made a snap substitution request no coke coffee instead - a hot beverage will make the grease go down smoother my server obligated me in silence and i acknowledged her generosity i took my tray and chose a seat by the front windows where parked cars would treat the diner with blinding light from their headlights but at least it was somewhat devoid of the cold glowing light emitting from the overhead florescent located in the rear of the restauranti sat down and emptied the contents of the bag fries and napkins no ketchup i tasted one of the fries to determine if they were worthy of ketchup-free consumption but alas no they were slightly cold with a texture that left an unpleasant lump in your throat i fetched several ketchup packets from the server kept under tight security behind the counter and returned to my tablespilling out the ketchup packets on the table i noticed dried droplets of ketchup on the plastic packets no turning back now i dug into my dinner with a overwhelming sadness for humanity and myself so to sum up:fries they left an odd taste of seafood or fish to my palette perhaps they co-mingled food in the fryer the luke-warm temparture made it a bit difficult to choke them down maybe some beer would have helped the processburger lifting the top bun of any burger you can tell right away if there was any love in the preparation though consumable the burger were not satisfying and tasted as if they sat under a heat lamp for too long also lukewarmcoffee i have tasted better coffee from a mechanical contraption in a corporate lunch room it was watery lukewarm and slightly sweet in a way that amplified everything that went wrong with the coffeeketchup packets looks like they were dropped on the floor at some point or recycled from a previous diner that did not use up all their ketchupcustomer service no complaints here it's not exactly the most cheery place to work servers were speedy helpful and responsive \n",
      " Review 2:  \n",
      " i've eaten at nougatine and abc kitchen numerous times but i have to admit that i keep coming back to this fine institution for a great meal that puts jgv's work to shame the french fries are an example of potatoes that have been prepared in a form that may be legitimately considered a work of art the gentle texture and crisp of the fries are testament to the corporation's impeccable execution of deep frying and the training that the personnel have undergone to achieve this perfection across the franchise is very much laudable my experience with the beef has been nothing short of flawless as i had requested the patty in my burger was prepared to a medium-rare and the subtle cheese and softly-grated onions danced with the rich ketchup and mustard on my tongue as i tasted thick and quality processed meat between two pieces of toasted and warm buns the acidity of the salted fries and that of the burger did not balance quite as i liked it though i was very much satisfied anyway i have to say that short of make-up sex i have had no better experience in my 20's to finish my three-course the apple pies were magnificent the texture of the apples led me to believe that they were in fact potatoes but i must commend macdonald for aptly managing their resources and overhead by taking advantage of economies of scale in purchasing only mass quantities of one ingredient as opposed to diversifying their portfolio; this may lead to higher margins in that the franchises would be able to provide more products in their menu eg genuine apple smoothies or apple-infused panacotta but would ultimately lead to higher operating costs in any case the dessert was very much satisfyingthough i paid only $600 for this meal i must have gained the delight of a $6000 meal cheers to an amazing institution for providing a meal aptly suited for all social castes \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each in index_simi.tolist():\n",
    "    review1 = mcd_review['review'][pair[each][0]]\n",
    "    review2 = mcd_review['review'][pair[each][1]]\n",
    "    similarity = result[each]\n",
    "    print( f'Simiarlity Score {similarity}', \"\\n\",\n",
    "           f'Review 1: \\n {review1}' , \"\\n\", \n",
    "           f'Review 2:  \\n {review2}', \n",
    "         \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part II:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  pd.read_csv(\"SMS_train.csv\", encoding = 'latin-1' )\n",
    "test = pd.read_csv(\"SMS_test.csv\", encoding = 'latin-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.loc[test['Label'] == 'Spam']) / len(test) ## The Spam rate is 0.608 for the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Label'].loc[train['Label'] == 'Non-Spam'] = 0\n",
    "train['Label'].loc[train['Label'] == 'Spam'] = 1\n",
    "train['Message_body'] = train['Message_body'].str.lower()\n",
    "\n",
    "test['Label'].loc[test['Label'] == 'Non-Spam'] = 0\n",
    "test['Label'].loc[test['Label'] == 'Spam'] = 1\n",
    "test['Message_body'] = test['Message_body'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_message(text):\n",
    "    text = text.replace('*', '')\n",
    "    text = re.sub(r':', r' ', text)\n",
    "    text = re.sub(r'<br />', r' ', text)\n",
    "    text = re.sub(r'\\b(Ã¼)\\b', r'u', text)\n",
    "    text = re.sub(r'\\S?(www|http)\\S+', r'URL', text)\n",
    "    text = re.sub(r'((\\d){9,})|(\\d+-\\d+-\\d+)|((\\d){3,}\\s?(\\d){3,}?\\s?(\\d){4,}?\\S+?)' , r'phone_number', text)\n",
    "\n",
    "    return text\n",
    "train['Message_body'] = train['Message_body'].apply(clean_message)\n",
    "test['Message_body'] = test['Message_body'].apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"ner\", \"tagger\", \"parser\", \"attribute_ruler\"])\n",
    "train['Message_body_nlp'] = train['Message_body'].apply(lambda x: nlp(x))\n",
    "test['Message_body_nlp'] = test['Message_body'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message = []\n",
    "for each in train['Message_body_nlp']:\n",
    "    cleaned_doc = \" \".join([token.lemma_ for token in each if not token.is_stop])\n",
    "    clean_message.append(cleaned_doc)\n",
    "clean_message\n",
    "train['clean'] = clean_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message = []\n",
    "for each in test['Message_body_nlp']:\n",
    "    cleaned_doc = \" \".join([token.lemma_ for token in each if not token.is_stop])\n",
    "    clean_message.append(cleaned_doc)\n",
    "clean_message\n",
    "test['clean'] = clean_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "      <th>Message_body_nlp</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>rofl. its true to its name</td>\n",
       "      <td>0</td>\n",
       "      <td>(rofl, ., its, true, to, its, name)</td>\n",
       "      <td>rofl . true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>the guy did some bitching but i acted like i'd...</td>\n",
       "      <td>0</td>\n",
       "      <td>(the, guy, did, some, bitching, but, i, acted,...</td>\n",
       "      <td>guy bitching acted like interested buying week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>pity,  was in mood for that. so...any other su...</td>\n",
       "      <td>0</td>\n",
       "      <td>(pity, ,,  , was, in, mood, for, that, ., so, ...</td>\n",
       "      <td>pity ,   mood . ... suggestions ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>will u b going to esplanade fr home?</td>\n",
       "      <td>0</td>\n",
       "      <td>(will, u, b, going, to, esplanade, fr, home, ?)</td>\n",
       "      <td>u b going esplanade fr home ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>this is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>1</td>\n",
       "      <td>(this, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "      <td>2nd time tried 2 contact u. u won Â£ 750 pound ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>121</td>\n",
       "      <td>7 wonders in my world 7th you 6th ur style 5th...</td>\n",
       "      <td>0</td>\n",
       "      <td>(7, wonders, in, my, world, 7th, you, 6th, ur,...</td>\n",
       "      <td>7 wonders world 7th 6th ur style 5th ur smile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>try to do something dear. you read something f...</td>\n",
       "      <td>0</td>\n",
       "      <td>(try, to, do, something, dear, ., you, read, s...</td>\n",
       "      <td>try dear . read exams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>sun ah... thk mayb can if dun have anythin on....</td>\n",
       "      <td>0</td>\n",
       "      <td>(sun, ah, ..., thk, mayb, can, if, dun, have, ...</td>\n",
       "      <td>sun ah ... thk mayb dun anythin ... thk book e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>symptoms when u are in love  \"1.u like listeni...</td>\n",
       "      <td>0</td>\n",
       "      <td>(symptoms, when, u, are, in, love,  , \", 1.u, ...</td>\n",
       "      <td>symptoms u love   \" 1.u like listening songs 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>125</td>\n",
       "      <td>great. have a safe trip. dont panic surrender ...</td>\n",
       "      <td>0</td>\n",
       "      <td>(great, ., have, a, safe, trip, ., do, nt, pan...</td>\n",
       "      <td>great . safe trip . nt panic surrender .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1082 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     S. No.                                       Message_body  Label  \\\n",
       "0         1                         rofl. its true to its name      0   \n",
       "1         2  the guy did some bitching but i acted like i'd...      0   \n",
       "2         3  pity,  was in mood for that. so...any other su...      0   \n",
       "3         4               will u b going to esplanade fr home?      0   \n",
       "4         5  this is the 2nd time we have tried 2 contact u...      1   \n",
       "..      ...                                                ...    ...   \n",
       "120     121  7 wonders in my world 7th you 6th ur style 5th...      0   \n",
       "121     122  try to do something dear. you read something f...      0   \n",
       "122     123  sun ah... thk mayb can if dun have anythin on....      0   \n",
       "123     124  symptoms when u are in love  \"1.u like listeni...      0   \n",
       "124     125  great. have a safe trip. dont panic surrender ...      0   \n",
       "\n",
       "                                      Message_body_nlp  \\\n",
       "0                  (rofl, ., its, true, to, its, name)   \n",
       "1    (the, guy, did, some, bitching, but, i, acted,...   \n",
       "2    (pity, ,,  , was, in, mood, for, that, ., so, ...   \n",
       "3      (will, u, b, going, to, esplanade, fr, home, ?)   \n",
       "4    (this, is, the, 2nd, time, we, have, tried, 2,...   \n",
       "..                                                 ...   \n",
       "120  (7, wonders, in, my, world, 7th, you, 6th, ur,...   \n",
       "121  (try, to, do, something, dear, ., you, read, s...   \n",
       "122  (sun, ah, ..., thk, mayb, can, if, dun, have, ...   \n",
       "123  (symptoms, when, u, are, in, love,  , \", 1.u, ...   \n",
       "124  (great, ., have, a, safe, trip, ., do, nt, pan...   \n",
       "\n",
       "                                                 clean  \n",
       "0                                          rofl . true  \n",
       "1    guy bitching acted like interested buying week...  \n",
       "2                    pity ,   mood . ... suggestions ?  \n",
       "3                        u b going esplanade fr home ?  \n",
       "4    2nd time tried 2 contact u. u won Â£ 750 pound ...  \n",
       "..                                                 ...  \n",
       "120  7 wonders world 7th 6th ur style 5th ur smile ...  \n",
       "121                              try dear . read exams  \n",
       "122  sun ah ... thk mayb dun anythin ... thk book e...  \n",
       "123  symptoms u love   \" 1.u like listening songs 2...  \n",
       "124           great . safe trip . nt panic surrender .  \n",
       "\n",
       "[1082 rows x 5 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train.append(test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Count Vectorizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea667d3982ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pipe_countVectorizer = Pipeline([('countV', CountVectorizer( stop_words = \"english\", token_pattern = '[a-z0-9+]')),\n\u001b[0m\u001b[1;32m      2\u001b[0m                          ('regression', LogisticRegression())])\n\u001b[1;32m      3\u001b[0m grid_logistic_parameter = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'countV__max_features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'countV__min_df'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe_countVectorizer = Pipeline([('countV', CountVectorizer( stop_words = \"english\", token_pattern = '[a-z0-9+]')),\n",
    "                         ('regression', LogisticRegression())])\n",
    "grid_logistic_parameter = {\n",
    "    'countV__max_features': (500, 1000,1500),\n",
    "    'countV__min_df': (0.001, 0.005, 0.01, 0.02),\n",
    "    'countV__ngram_range':((1,1),(1,2),(1,3)),\n",
    "    'regression__C': (0.1,1,10),\n",
    "    'regression__solver': ( 'liblinear', 'sag', 'lbfgs')}\n",
    "\n",
    "X = df['clean']\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 957, shuffle=False)\n",
    "\n",
    "grid_search = GridSearchCV(pipe_countVectorizer, \n",
    "                           param_grid=grid_logistic_parameter,\n",
    "                           cv=10, n_jobs=-1, scoring='accuracy')\n",
    "model_countV = grid_search.fit(X_train,y_train)\n",
    "y_predicted_countV = model_countV.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countV__max_features': 500,\n",
       " 'countV__min_df': 0.001,\n",
       " 'countV__ngram_range': (1, 2),\n",
       " 'regression__C': 0.1,\n",
       " 'regression__solver': 'liblinear'}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_countV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_countV.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888157894736842"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score( y_test , y_predicted_countV  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  0],\n",
       "       [17, 59]])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test , y_predicted_countV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sms . ac blind date 4u !   rodds1 21 / m aberdeen , united kingdom . check http //img . sms . ac / w / icmb3cktz8r7!-4 blind dates send hide',\n",
       "        'ringtone club   uk singles chart mobile week choose quality ringtone ! message free charge .',\n",
       "        'hey horny want chat naked text hot 69698 text charged 150pm unsubscribe text stop 69698',\n",
       "        'u phone_number secret admirer looking 2 contact u - find rreveal thinks ur special - phone_number',\n",
       "        'text & meet sexy today . u find date flirt u. join 4 10p . reply & age eg sam 25 . 18 -msg recd@thirtyeight pence',\n",
       "        'u meet ur dream partner soon ? ur career 2 flyng start ? 2 find free , txt horo followed ur star sign , e. g. horo aries',\n",
       "        'unique ? find 30th august . url',\n",
       "        'bangbabes ur order way . u receive service msg 2 download ur content . u , goto wap . bangb . tv ur mobile internet / service menu',\n",
       "        'rcv msgs chat svc . free hardcore services text   69988 u u age verify yr network & try',\n",
       "        'want 2 laid tonight ? want real dogging locations sent direct 2 ur mob ? join uk largest dogging network bt txting gravel 69888 ! nt . ec2a . 31p.msg@150p',\n",
       "        'sunshine quiz wkly q ! win sony dvd player u know country algarve ? txt ansr 82277 . Â£ 1.50 sp tyrone',\n",
       "        'hear new \" divorce barbie \" ? comes ken stuff !',\n",
       "        'sms . ac sptv   new jersey devils detroit red wings play ice hockey . correct incorrect ? end ? reply end sptv',\n",
       "        'thanks subscription ringtone uk mobile charged Â£ 5 / month confirm replying yes . reply charged',\n",
       "        'xxxmobilemovieclub   use credit , click wap link txt message click > > http //wap . xxxmobilemovieclub.com?n=qjkgighjjgcbl',\n",
       "        'freemsg hey darling 3 week word ! like fun ? tb ok ! xxx std chgs send , Â£ 1.50 rcv',\n",
       "        'contract mobile 11 mnths ? latest motorola , nokia etc . free ! double mins & text orange tariffs . text yes callback , remove records']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongprediction = np.nonzero(y_predicted_countV != y_test)\n",
    "np.take(np.array(test['clean']),wrongprediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - When we look at the wrong reviews above from the Count Vectorization model, we can see that the model fails to classify corretly for spam messages invovle 'message', 'text back/txt back', 'reply'. Thus, I may need to use regex to clean them into one words so that Count Vectorization may achieve better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "pipe_tfidf = Pipeline([('tfidf', TfidfVectorizer( stop_words = \"english\", token_pattern = '[?!a-z0-9+]')),\n",
    "                         ('regression', LogisticRegression())])\n",
    "tfidf_parameter = {\n",
    "    'tfidf__max_features': (500, 1000,1500, 2000),\n",
    "    'tfidf__min_df': (0.001, 0.005, 0.01),\n",
    "    'tfidf__ngram_range':((1,1),(1,2),(1,3)),\n",
    "    'regression__C': (0.1,1,10),\n",
    "    'regression__solver': ( 'liblinear', 'sag', 'lbfgs')}\n",
    "\n",
    "X = df['clean']\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 957, shuffle=False)\n",
    "\n",
    "grid_search_2 = GridSearchCV(pipe_tfidf, \n",
    "                           param_grid= tfidf_parameter,\n",
    "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
    "model_tfidf = grid_search_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regression__C': 10,\n",
       " 'regression__solver': 'lbfgs',\n",
       " 'tfidf__max_features': 500,\n",
       " 'tfidf__min_df': 0.001,\n",
       " 'tfidf__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score( y_test , y_predicted_tfidf  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  0],\n",
       "       [12, 64]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test , y_predicted_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sms . ac blind date 4u !   rodds1 21 / m aberdeen , united kingdom . check http //img . sms . ac / w / icmb3cktz8r7!-4 blind dates send hide',\n",
       "        'ringtone club   uk singles chart mobile week choose quality ringtone ! message free charge .',\n",
       "        'text & meet sexy today . u find date flirt u. join 4 10p . reply & age eg sam 25 . 18 -msg recd@thirtyeight pence',\n",
       "        'u meet ur dream partner soon ? ur career 2 flyng start ? 2 find free , txt horo followed ur star sign , e. g. horo aries',\n",
       "        'unique ? find 30th august . url',\n",
       "        'bangbabes ur order way . u receive service msg 2 download ur content . u , goto wap . bangb . tv ur mobile internet / service menu',\n",
       "        'rcv msgs chat svc . free hardcore services text   69988 u u age verify yr network & try',\n",
       "        'hear new \" divorce barbie \" ? comes ken stuff !',\n",
       "        'sms . ac sptv   new jersey devils detroit red wings play ice hockey . correct incorrect ? end ? reply end sptv',\n",
       "        'thanks subscription ringtone uk mobile charged Â£ 5 / month confirm replying yes . reply charged',\n",
       "        'xxxmobilemovieclub   use credit , click wap link txt message click > > http //wap . xxxmobilemovieclub.com?n=qjkgighjjgcbl',\n",
       "        'freemsg hey darling 3 week word ! like fun ? tb ok ! xxx std chgs send , Â£ 1.50 rcv']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongprediction = np.nonzero(y_predicted_tfidf != y_test)\n",
    "np.take(np.array(test['clean']),wrongprediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the result above, we can see that TF-IDF appraoch fails to classify those spam with email address '@' and monetary symbols like 'Â£' correctly. I should try to transform all the email format into one token and keep all the monetary symbols into the vector transfomration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[49,  0],\n",
       "       [17, 59]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['message_vector'] = train['Message_body_nlp'].apply(lambda x: x.vector)\n",
    "test['message_vector'] = test['Message_body_nlp'].apply(lambda x: x.vector)\n",
    "X_train = np.array(train.message_vector.tolist())\n",
    "X_test = np.array(test.message_vector.tolist())\n",
    "y_train = train['Label'].values\n",
    "y_test = test['Label'].values\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "model_w2v = logistic.fit(X_train,y_train)\n",
    "y_predicted = model_w2v.predict(X_test)\n",
    "confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train.message_vector.tolist())\n",
    "X_test = np.array(test.message_vector.tolist())\n",
    "y_train = train['Label'].values\n",
    "y_test = test['Label'].values\n",
    "pipe_w2v = Pipeline([('regression', LogisticRegression())])\n",
    "logistic_parameter = {\n",
    "    'regression__C': (0.1,1,10,100),\n",
    "    'regression__solver': ( 'liblinear', 'sag', 'lbfgs')}\n",
    "grid_search_w2v = GridSearchCV(pipe_w2v, param_grid = logistic_parameter, cv = 5, verbose=True, n_jobs=-1)\n",
    "w2v_grid_model = grid_search_w2v.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regression__C': 10, 'regression__solver': 'liblinear'}"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_grid_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_w2v = w2v_grid_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9240064446831364"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score( y_test , y_predicted_w2v  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48,  1],\n",
       "       [10, 66]])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test , y_predicted_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['contacted dating service entered phone fancy ! find landline phone_number . pobox12n146tf150p',\n",
       "        'u phone_number secret admirer looking 2 contact u - find rreveal thinks ur special - phone_number',\n",
       "        'text & meet sexy today . u find date flirt u. join 4 10p . reply & age eg sam 25 . 18 -msg recd@thirtyeight pence',\n",
       "        'u meet ur dream partner soon ? ur career 2 flyng start ? 2 find free , txt horo followed ur star sign , e. g. horo aries',\n",
       "        'freemsg replied text ? randy , sexy , female live local . luv hear u. netcollex ltd phone_numberp msg reply stop end',\n",
       "        'rcv msgs chat svc . free hardcore services text   69988 u u age verify yr network & try',\n",
       "        'hear new \" divorce barbie \" ? comes ken stuff !',\n",
       "        'sms . ac sptv   new jersey devils detroit red wings play ice hockey . correct incorrect ? end ? reply end sptv',\n",
       "        'freemsg hey darling 3 week word ! like fun ? tb ok ! xxx std chgs send , Â£ 1.50 rcv',\n",
       "        'running managed 5 minutes needed oxygen ! resort roller option !',\n",
       "        'u secret admirer looking 2 contact u - find rreveal thinks ur special - phone_number - stopsms - phone_numberppm']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrongprediction = np.nonzero(y_predicted_w2v != y_test)\n",
    "np.take(np.array(test['clean']),wrongprediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Word to Vector Embedding after gridsearch tuning acheives the highest accuray rate and roc score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
